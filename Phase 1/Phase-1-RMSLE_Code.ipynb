{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RMSLE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7Z9zyvupgC4",
        "outputId": "185c6f0c-b8a8-471d-b8de-5c21c479db3e"
      },
      "source": [
        "#Importing standard library math\n",
        "import math\n",
        "import numpy as np\n",
        "#creating custom function for RMSLE calculation\n",
        "def My_RMSLE(pred, trgt):\n",
        "  #initializing tot variable, which will hold Total value\n",
        "  tot = 0 \n",
        "\n",
        "  #Looping through the passback variable pred\n",
        "  for k in range(len(pred)):\n",
        "    #for each predicted value and corresponding target value is picked below\n",
        "    #logarithm applied\n",
        "    LPred= np.log1p(pred[k]+1)\n",
        "    LTarg = np.log1p(trgt[k] + 1)\n",
        "\n",
        "    #checking if none of the Pred & Target value is null then\n",
        "    #squaring the differences    \n",
        "    if not (math.isnan(LPred)) and  not (math.isnan(LTarg)):    \n",
        "       tot = tot + ((LPred-LTarg) **2)  \n",
        "            \n",
        "  #finding our average value and then doing Sqrroot & picking final value      \n",
        "  tot = tot / len(pred)        \n",
        "  return np.sqrt(tot)\n",
        "\n",
        "y_pred = [2,5,6,1,7,9]\n",
        "y      = [2.5,6,5,1,7,8.5]\n",
        "print ('My custom RMSLE: ' + str(My_RMSLE(y_pred,y)))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My custom RMSLE: 0.09282440499458498\n"
          ]
        }
      ]
    }
  ]
}